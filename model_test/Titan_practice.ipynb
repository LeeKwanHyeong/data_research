{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-05T03:15:31.026520Z",
     "start_time": "2025-08-05T03:15:28.780131Z"
    }
   },
   "source": [
    "import traceback\n",
    "\n",
    "import polars as pl\n",
    "import sys\n",
    "import torch\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from lifelines import WeibullFitter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from models.Titans import Model\n",
    "from utils.custom_losses import CustomLoss\n",
    "from utils.lstf_feature_maker.piecewise_linear_regression import PiecewiseLinearRegression\n",
    "from utils.lstf_feature_maker.weibull import WeibullFeatureMaker\n",
    "\n",
    "'''\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "https://developer.nvidia.com/cuda-12-8-0-download-archive\n",
    "'''\n",
    "\n",
    "MAC_DIR = '../data/'\n",
    "WINDOW_DIR = 'C:/Users/USER/PycharmProjects/research/data/'\n",
    "\n",
    "if sys.platform == 'win32':\n",
    "    DIR = WINDOW_DIR\n",
    "    print(torch.cuda.is_available())\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.version.cuda)\n",
    "    print(torch.__version__)\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.__version__)\n",
    "else:\n",
    "    DIR = MAC_DIR\n",
    "\n",
    "tb_bas_oper_part_mst = (pl.read_parquet(DIR + 'tb_bas_oper_part_mst.parquet')\n",
    "                        .select(['OPER_PART_NO', 'OPER_PART_NM'])\n",
    "                        .rename({'OPER_PART_NO': 'oper_part_no', 'OPER_PART_NM': 'oper_part_nm'}))\n",
    "tb_dyn_fcst_demand = (pl.read_parquet(DIR + 'tb_dyn_fcst_dmnd.parquet')\n",
    "                      .select(['PART_NO', 'DMND_QTY', 'DMND_DT', 'OPER_PART_NO'])\n",
    "                      .rename({'PART_NO': 'part_no', 'OPER_PART_NO': 'oper_part_no', 'DMND_DT': 'demand_dt', 'DMND_QTY': 'demand_qty'})\n",
    "                      .select(['part_no', 'oper_part_no', 'demand_dt', 'demand_qty']))\n",
    "tb_dyn_fcst_demand_sellout = (pl.read_parquet(DIR + 'tb_dyn_fcst_dmnd_sellout.parquet')\n",
    "                              .select(['PART_NO', 'DMND_QTY', 'DMND_DT', 'OPER_PART_NO'])\n",
    "                              .rename({'PART_NO': 'part_no', 'OPER_PART_NO': 'oper_part_no', 'DMND_DT': 'demand_dt', 'DMND_QTY': 'demand_qty'})\n",
    "                              .select(['part_no', 'oper_part_no', 'demand_dt', 'demand_qty']))"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T03:15:40.401843Z",
     "start_time": "2025-08-05T03:15:31.754349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.date_util import DateUtil\n",
    "\n",
    "dyn_fcst_demand = tb_dyn_fcst_demand.with_columns([\n",
    "    pl.col('demand_dt').cast(pl.Int64).map_elements(DateUtil.yyyymmdd_to_date, return_dtype = pl.Date)\n",
    "])\n",
    "\n",
    "dyn_demand_sellout = tb_dyn_fcst_demand_sellout.with_columns([\n",
    "    pl.col('demand_dt').cast(pl.Int64).map_elements(DateUtil.yyyymmdd_to_date, return_dtype = pl.Date)\n",
    "])\n",
    "\n",
    "dyn_fcst_demand"
   ],
   "id": "397fb1d36e62beca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (1_723_020, 4)\n",
       "┌───────────────┬───────────────┬────────────┬────────────┐\n",
       "│ part_no       ┆ oper_part_no  ┆ demand_dt  ┆ demand_qty │\n",
       "│ ---           ┆ ---           ┆ ---        ┆ ---        │\n",
       "│ str           ┆ str           ┆ date       ┆ f64        │\n",
       "╞═══════════════╪═══════════════╪════════════╪════════════╡\n",
       "│ T4240-71102BB ┆ T4240-71102BB ┆ 2018-01-01 ┆ 3.0        │\n",
       "│ T5210-34402   ┆ T5210-34402   ┆ 2018-01-01 ┆ 1.0        │\n",
       "│ T5210-30081   ┆ T5210-30081   ┆ 2018-01-01 ┆ 1.0        │\n",
       "│ T5210-65661   ┆ T5210-65661   ┆ 2018-01-01 ┆ 1.0        │\n",
       "│ T5210-66472   ┆ T5210-66472   ┆ 2018-01-01 ┆ 1.0        │\n",
       "│ …             ┆ …             ┆ …          ┆ …          │\n",
       "│ U3215-52203   ┆ U3215-52203   ┆ 2024-02-05 ┆ 30.0       │\n",
       "│ T5710-69252   ┆ T5710-69252   ┆ 2024-02-05 ┆ 2.0        │\n",
       "│ DYD1-O07      ┆ DYD1-O07      ┆ 2024-02-05 ┆ 4.0        │\n",
       "│ T2198-69775   ┆ T2198-69775   ┆ 2024-02-05 ┆ 6.0        │\n",
       "│ TC26-0416B    ┆ TC26-0416B    ┆ 2024-02-05 ┆ 2.0        │\n",
       "└───────────────┴───────────────┴────────────┴────────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_723_020, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>part_no</th><th>oper_part_no</th><th>demand_dt</th><th>demand_qty</th></tr><tr><td>str</td><td>str</td><td>date</td><td>f64</td></tr></thead><tbody><tr><td>&quot;T4240-71102BB&quot;</td><td>&quot;T4240-71102BB&quot;</td><td>2018-01-01</td><td>3.0</td></tr><tr><td>&quot;T5210-34402&quot;</td><td>&quot;T5210-34402&quot;</td><td>2018-01-01</td><td>1.0</td></tr><tr><td>&quot;T5210-30081&quot;</td><td>&quot;T5210-30081&quot;</td><td>2018-01-01</td><td>1.0</td></tr><tr><td>&quot;T5210-65661&quot;</td><td>&quot;T5210-65661&quot;</td><td>2018-01-01</td><td>1.0</td></tr><tr><td>&quot;T5210-66472&quot;</td><td>&quot;T5210-66472&quot;</td><td>2018-01-01</td><td>1.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;U3215-52203&quot;</td><td>&quot;U3215-52203&quot;</td><td>2024-02-05</td><td>30.0</td></tr><tr><td>&quot;T5710-69252&quot;</td><td>&quot;T5710-69252&quot;</td><td>2024-02-05</td><td>2.0</td></tr><tr><td>&quot;DYD1-O07&quot;</td><td>&quot;DYD1-O07&quot;</td><td>2024-02-05</td><td>4.0</td></tr><tr><td>&quot;T2198-69775&quot;</td><td>&quot;T2198-69775&quot;</td><td>2024-02-05</td><td>6.0</td></tr><tr><td>&quot;TC26-0416B&quot;</td><td>&quot;TC26-0416B&quot;</td><td>2024-02-05</td><td>2.0</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T03:15:41.791113Z",
     "start_time": "2025-08-05T03:15:41.317854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dyn_fcst = (dyn_fcst_demand\n",
    "                .join(tb_bas_oper_part_mst, on = 'oper_part_no', how = 'left')\n",
    "                .select(['oper_part_no', 'oper_part_nm', 'demand_dt','demand_qty'])\n",
    "                .sort(['oper_part_no', 'demand_dt'])\n",
    "                .with_columns([\n",
    "                    pl.col('demand_qty').cum_sum().over('oper_part_no').alias('cumsum_qty')\n",
    "                ])\n",
    "              )\n",
    "dyn_demand = (dyn_demand_sellout.join(tb_bas_oper_part_mst, on = 'oper_part_no', how = 'left')\n",
    "                    .select(['oper_part_no', 'oper_part_nm', 'demand_dt', 'demand_qty'])\n",
    "                    .sort(['oper_part_no', 'demand_dt'])\n",
    "                    .with_columns([\n",
    "                        pl.col('demand_qty').cum_sum().over('oper_part_no').alias('cumsum_qty')\n",
    "                 ])\n",
    "               )"
   ],
   "id": "fe8eb46a87844456",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T03:15:46.238425Z",
     "start_time": "2025-08-05T03:15:43.095028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lookback_window = 40\n",
    "horizon = 20\n",
    "df_grouped = (dyn_demand\n",
    "                .select(['oper_part_no', 'demand_dt', 'demand_qty', 'cumsum_qty'])\n",
    "                .sort(['oper_part_no', 'demand_dt'])\n",
    "                .with_columns(pl.col('demand_dt').map_elements(DateUtil.date_to_yyyymm, return_dtype = pl.Int64).alias('demand_dt'))\n",
    "                .select('oper_part_no', 'demand_dt', 'demand_qty')\n",
    "                .group_by(['oper_part_no', 'demand_dt'])\n",
    "                .agg(pl.col('demand_qty').sum().alias('demand_qty'))\n",
    "                .sort(['oper_part_no', 'demand_dt'])\n",
    "              )\n",
    "\n",
    "df_grouped"
   ],
   "id": "60bd3420970e7dcb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (867_077, 3)\n",
       "┌──────────────┬───────────┬────────────┐\n",
       "│ oper_part_no ┆ demand_dt ┆ demand_qty │\n",
       "│ ---          ┆ ---       ┆ ---        │\n",
       "│ str          ┆ i64       ┆ f64        │\n",
       "╞══════════════╪═══════════╪════════════╡\n",
       "│ 0001-1001    ┆ 201803    ┆ 5.0        │\n",
       "│ 0001-1001    ┆ 201811    ┆ 7.0        │\n",
       "│ 0001-1001    ┆ 202002    ┆ 120.0      │\n",
       "│ 0001-1001    ┆ 202003    ┆ 2.0        │\n",
       "│ 0001-1001    ┆ 202305    ┆ 2.0        │\n",
       "│ …            ┆ …         ┆ …          │\n",
       "│ ZZ90239      ┆ 202010    ┆ 1.0        │\n",
       "│ ZZ90239      ┆ 202111    ┆ 1.0        │\n",
       "│ ZZ90239      ┆ 202306    ┆ 1.0        │\n",
       "│ ZZ90239      ┆ 202411    ┆ 1.0        │\n",
       "│ ZZ90239      ┆ 202606    ┆ 1.0        │\n",
       "└──────────────┴───────────┴────────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (867_077, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>oper_part_no</th><th>demand_dt</th><th>demand_qty</th></tr><tr><td>str</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;0001-1001&quot;</td><td>201803</td><td>5.0</td></tr><tr><td>&quot;0001-1001&quot;</td><td>201811</td><td>7.0</td></tr><tr><td>&quot;0001-1001&quot;</td><td>202002</td><td>120.0</td></tr><tr><td>&quot;0001-1001&quot;</td><td>202003</td><td>2.0</td></tr><tr><td>&quot;0001-1001&quot;</td><td>202305</td><td>2.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;ZZ90239&quot;</td><td>202010</td><td>1.0</td></tr><tr><td>&quot;ZZ90239&quot;</td><td>202111</td><td>1.0</td></tr><tr><td>&quot;ZZ90239&quot;</td><td>202306</td><td>1.0</td></tr><tr><td>&quot;ZZ90239&quot;</td><td>202411</td><td>1.0</td></tr><tr><td>&quot;ZZ90239&quot;</td><td>202606</td><td>1.0</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T03:15:47.670197Z",
     "start_time": "2025-08-05T03:15:47.664815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiPartTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df: pl.DataFrame, lookback: int, horizon: int):\n",
    "        self.samples: list[tuple[np.ndarray, np.ndarray]] = []\n",
    "        self.part_ids: list[str] = []\n",
    "\n",
    "        grouped = df.sort(['oper_part_no', 'demand_dt']).partition_by('oper_part_no')\n",
    "\n",
    "        for df in grouped:\n",
    "            part_no = df['oper_part_no'][0]\n",
    "            series = df['demand_qty'].to_numpy()\n",
    "\n",
    "            for i in range(len(series) - lookback - horizon):\n",
    "                x_seq = series[i:i + lookback]\n",
    "                y_seq = series[i + lookback: i + lookback + horizon]\n",
    "                self.samples.append((x_seq, y_seq))\n",
    "                self.part_ids.append(part_no)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_seq, y_seq = self.samples[idx]\n",
    "        return torch.tensor(x_seq, dtype = torch.float32).unsqueeze(-1),torch.tensor(y_seq, dtype = torch.float32), self.part_ids[idx]"
   ],
   "id": "948e2f91193067a6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T03:17:15.473518Z",
     "start_time": "2025-08-05T03:17:15.457819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10, lr=1e-3, device='cuda'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch, _ in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x_batch)\n",
    "            loss = loss_fn(pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val, _ in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                pred = model(x_val)\n",
    "                val_loss += loss_fn(pred, y_val).item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
   ],
   "id": "be0c3f54d441916",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T03:21:21.325393Z",
     "start_time": "2025-08-05T03:17:17.343273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.Titans import TestTimeMemoryManager\n",
    "\n",
    "lookback = 48\n",
    "horizon = 12\n",
    "\n",
    "dataset = MultiPartTimeSeriesDataset(df_grouped, lookback, horizon)\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 256, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 256)\n",
    "\n",
    "model = Model(\n",
    "        input_dim=1,\n",
    "        d_model=128,\n",
    "        n_layers=3,\n",
    "        n_heads=4,\n",
    "        d_ff=256,\n",
    "        contextual_mem_size=64,\n",
    "        persistent_mem_size=32,  # 메모리 용량 확장\n",
    "        output_horizon=horizon\n",
    "    )\n",
    "\n",
    "    # ✅ 학습\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_model(model, train_loader, val_loader, epochs=10, lr=1e-3, device=device)\n",
    "\n",
    "    # ✅ 특정 파트 예측 + Test-Time Memory Adaptation\n",
    "test_part = \"0001-1001\"\n",
    "test_series = (\n",
    "    df_grouped.filter(pl.col(\"oper_part_no\") == test_part)\n",
    "        .sort(\"demand_dt\")[\"demand_qty\"].to_numpy()\n",
    ")\n",
    "x_new = torch.tensor(test_series[-(lookback+horizon):-horizon]).unsqueeze(0).unsqueeze(-1)\n",
    "y_new = torch.tensor(test_series[-horizon:]).unsqueeze(0)\n",
    "\n",
    "    # Memory Adaptation\n",
    "ttm = TestTimeMemoryManager(model)\n",
    "ttm.add_context(x_new)\n",
    "loss_after_adapt = ttm.adapt(x_new, y_new, steps=3)\n",
    "print(f\"Adaptation Loss after TTM ({test_part}): {loss_after_adapt:.4f}\")\n",
    "\n",
    "    # ✅ 예측 결과\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(x_new)\n",
    "print(\"Predicted:\", pred.cpu().numpy().flatten())\n",
    "print(\"Actual:\", y_new.cpu().numpy().flatten())"
   ],
   "id": "4f9d1f130289c33f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1658465.3041 | Val Loss: 3139851.7385\n",
      "Epoch 2/10 | Train Loss: 1645888.6341 | Val Loss: 3123896.7920\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     25\u001B[39m     \u001B[38;5;66;03m# ✅ 학습\u001B[39;00m\n\u001B[32m     26\u001B[39m device = \u001B[33m\"\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.cuda.is_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1e-3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     29\u001B[39m     \u001B[38;5;66;03m# ✅ 특정 파트 예측 + Test-Time Memory Adaptation\u001B[39;00m\n\u001B[32m     30\u001B[39m test_part = \u001B[33m\"\u001B[39m\u001B[33m0001-1001\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 12\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_loader, val_loader, epochs, lr, device)\u001B[39m\n\u001B[32m     10\u001B[39m x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n\u001B[32m     11\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m pred = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m loss = loss_fn(pred, y_batch)\n\u001B[32m     14\u001B[39m loss.backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/data_research/models/Titans.py:23\u001B[39m, in \u001B[36mModel.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m     encoded = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     24\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.output_proj(encoded[:, -\u001B[32m1\u001B[39m, :])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/data_research/layers/Titans_Memory.py:116\u001B[39m, in \u001B[36mMemoryEncoder.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    114\u001B[39m x = \u001B[38;5;28mself\u001B[39m.input_proj(x)\n\u001B[32m    115\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.layers:\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m     x = \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    117\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/data_research/layers/Titans_Memory.py:99\u001B[39m, in \u001B[36mMemoryTransformerBlock.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     97\u001B[39m attn_out = \u001B[38;5;28mself\u001B[39m.attn(x)\n\u001B[32m     98\u001B[39m x = \u001B[38;5;28mself\u001B[39m.norm1(x + \u001B[38;5;28mself\u001B[39m.dropout(attn_out))\n\u001B[32m---> \u001B[39m\u001B[32m99\u001B[39m ffn_out = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mffn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    100\u001B[39m x = \u001B[38;5;28mself\u001B[39m.norm2(x + \u001B[38;5;28mself\u001B[39m.dropout(ffn_out))\n\u001B[32m    101\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/data_research/layers/Titans_Memory.py:84\u001B[39m, in \u001B[36mPositionWiseFFN.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     83\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m---> \u001B[39m\u001B[32m84\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.linear2(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlinear1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/dropout.py:70\u001B[39m, in \u001B[36mDropout.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m     69\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m---> \u001B[39m\u001B[32m70\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:1425\u001B[39m, in \u001B[36mdropout\u001B[39m\u001B[34m(input, p, training, inplace)\u001B[39m\n\u001B[32m   1422\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m p < \u001B[32m0.0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m p > \u001B[32m1.0\u001B[39m:\n\u001B[32m   1423\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdropout probability has to be between 0 and 1, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mp\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m   1424\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[32m-> \u001B[39m\u001B[32m1425\u001B[39m     _VF.dropout_(\u001B[38;5;28minput\u001B[39m, p, training) \u001B[38;5;28;01mif\u001B[39;00m inplace \u001B[38;5;28;01melse\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1426\u001B[39m )\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "38c66b057b620787",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_grouped",
   "id": "f7930ec9f218a849",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c9cbf4189b3ed269",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
