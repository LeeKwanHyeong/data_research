{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3911924358.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[1], line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    class PatchMixerLayer(nn.Module):\u001B[0m\n\u001B[0m                                     ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class PatchMixerLayer(nn.Module):\n",
    "    def __init__(self, dim, a, kernel_size = 8):\n",
    "        super().__init__()\n",
    "        self.Resnet = nn.Sequential(\n",
    "            nn.Conv1d(dim, dim, kernel_size = kernel_size, groups = dim, padding = 'same')\n",
    "            nn.GELU()\n",
    "            nn.BatchNorm1d(dim)\n",
    "        )\n",
    "\n",
    "        self.Conv_1x1 = nn.Sequential(\n",
    "            nn.Conv1d(dim, a, kernel_size = 1),\n",
    "            nn.GELU()\n",
    "            nn.BatchNorm1d(a)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x += self.Resnet(x)\n",
    "        x = self.Conv_1x1(x)\n",
    "        return x\n",
    "\n",
    "class PatchMixerBackbone(nn.Module):\n",
    "    def __init__(self, configs, revin = True, affine = True, subtract_last = False):\n",
    "        super().__init__()\n",
    "        self.n_vals = configs.enc_in\n",
    "        self.lookback = configs.seq_len\n",
    "        self.forecasting = configs.pred_len\n",
    "        self.patch_size = configs.patch_len\n",
    "        self.stride = configs.stride\n",
    "        self.kernel_size: int = configs.mixer_kernel_size\n",
    "\n",
    "        self.PatchMixer_blocks = nn.ModuleList([])\n",
    "        self.padding_patch_layer = nn.ReplicationPad1d((0, self.stride))\n",
    "        self.patch_num = int((self.lookback - self.patch_size) / self.stride + 1) + 1\n",
    "        self.a = self.patch_num\n",
    "        self.d_model = configs.d_model\n",
    "        self.dropout_rate = configs.head_dropout\n",
    "        self.depth = configs.e_layers\n",
    "\n",
    "        for _ in range(self.depth):\n",
    "            self.PatchMixer_blocks.append(PatchMixerLayer(dim = self.patch_num, a = self.a, kernel_size = self.kernel_size))\n",
    "            self.W_P = nn.Linear(self.patch_size, self.d_model)\n",
    "            self.flatten = nn.Flatten(start_dim = -2)\n",
    "\n",
    "            self.dropout = nn.Dropout(self.dropout_rate)\n",
    "            self.revin = revin\n",
    "            if self.revin:\n",
    "                self.revin_layer = RevIn(self.n_vals, affine = affine, subtract_last = subtract_last)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        n_vars = x.shape[-1]\n",
    "\n",
    "        if self.revin:\n",
    "            x = self.revin_layer(x, 'norm')\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x_lookback = self.padding_patch_layer(x)\n",
    "        x = x_lookback.unfold(dimension = -1, size = self.patch_size, step = self.stride)\n",
    "\n",
    "        x = self.W_P(x)\n",
    "        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for PatchMixer_block in self.PatchMixer_blocks:\n",
    "            x = PatchMixer_block(x)\n",
    "\n",
    "        # Global representation (flatten for merge)\n",
    "        x = self.flatten(x) # shape: (batch * n_vars, a * d_models)\n",
    "        x = x.view(bs, n_vars, -1)\n",
    "        x = x.mean(dim = 1) # aggregate across variables\n",
    "        return x # shape: (batch, patch_num * d_models)\n",
    "\n",
    "# Full Model: PatchMixer + Feature Branch\n",
    "class PatchMixerFeatureModel(nn.Module):\n",
    "    def __init__(self, configs, feature_dim = 7):\n",
    "        super().__init__()\n",
    "        self.backbone = PatchMixerBackbone(configs)\n",
    "\n",
    "        # Feature Branch\n",
    "        self.feature_mlp = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, feature_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Combined Head\n",
    "        patch_repr_dim = self.backbone.a * self.backbone.d_model\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(patch_repr_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        combined_dim = 128 + feature_dim\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, configs.pred_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, ts_input, feature_input):\n",
    "        # 1. PatchMixer Backbone으로 시계열 입력 처리\n",
    "        patch_repr = self.backbone(ts_input) # shape: (batch, patch_repr_dim)\n",
    "        # print(f'patch_repr: {patch_repr.shape}')\n",
    "\n",
    "        # 2. Projection layer 차원 축소\n",
    "        patch_repr_proj = self.proj(patch_repr)\n",
    "        # print(f'patch_repr_proj: {patch_repr_proj.shape}')\n",
    "\n",
    "        # 3. Feature branch processing\n",
    "        feature_repr = self.feature_mlp(feature_input)\n",
    "        # print(f'feature_repr: {feature_repr.shape}')\n",
    "\n",
    "        # 4. Concatenate\n",
    "        combined = torch.cat([patch_repr_proj, feature_repr], dim = 1)\n",
    "        # print(f'combined: {combined.shape}')\n",
    "\n",
    "        # 5. Fully connected layers -> 최종 예측\n",
    "        out = self.fc(combined) # shape: (batch, pred_len)\n",
    "        # print(f'out: {out.shape}')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclaer_X = MinMaxScaler()\n",
    "X_ts = np.vstack(feature_result['X_ts'].to_list())\n",
    "X_ts_scaled = scaler_X.fit_transform(X_ts.reshape(-1, 1)).reshape(X_ts.shape)\n",
    "\n",
    "y_ts = np.vstack(feature_result['y_ts'].to_list())\n",
    "y_ts_log = np.log1p(y_ts)\n",
    "scaler_y = MinMaxScaler()\n",
    "y_ts_scaled = scaler_y.fit_transform(y_ts_log.reshape(-1, 1)).reshape(y_ts_log.shape)\n",
    "\n",
    "scaler_feat = StandardScaler()\n",
    "X_feat = np.vstack(feature_result['X_features'].to_list())\n",
    "X_features_scaled = scaler_feat.fit_transform(X_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemandDataset(Dataset):\n",
    "    def __init__(self, X_ts, X_feat, y_ts):\n",
    "        self.X_ts = torch.tensor(X_ts, dtype = torch.float32).unsqueeze(-1)\n",
    "        self.X_feat = torch.tensor(X_feat, dtype = torch.float32)\n",
    "        self.y_ts = torch.tensor(y_ts, dtype = torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_ts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_ts[idx], self.X_feat[idx], self.y_ts[idx]\n",
    "\n",
    "X_train, X_val, F_train, F_val, y_train, y_val = train_test_split(X_ts_scaled, X_features_scaled, y_ts_scaled, test_size = 0.2, random_state = 42)\n",
    "\n",
    "train_dataset = DemandDataset(X_train, F_train, y_train)\n",
    "val_dataset = DemandDataset(X_val, F_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 32, shuffle = True)\n",
    "\n",
    "from domain.models.dl_models.models.PatchMixer import PatchMixerFeatureModel\n",
    "\n",
    "class Config:\n",
    "    enc_in = 1\n",
    "    seq_len = lookback_window\n",
    "    pred_len = horizon\n",
    "    patch_len = 16\n",
    "    stride = 8\n",
    "    mixer_kernel_size = 8\n",
    "    # d_model = 32\n",
    "    d_model = 16\n",
    "    head_dropout = 0.1\n",
    "    # e_layers = 3\n",
    "    e_layers = 2\n",
    "\n",
    "config = Config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = PatchMixerFeatureModel(config).to(device)\n",
    "\n",
    "def custom_loss(pred, target, alpha = 0.7):\n",
    "    baes_loss = nn.functional.smooth_l1_loss(pred, target)\n",
    "    # rel_error = torch.abs(pred_target) / (target + 1e-6)\n",
    "    # rel_loss = rel_error.mean()\n",
    "    # return alpha * base_loss + (1-alpha) * rel_loss\n",
    "    return base_loss\n",
    "\n",
    "beta = 0.001\n",
    "def combined_loss(pred, target):\n",
    "    base = custom_loss(pred, target)\n",
    "    smoothness = torch.mean(torch.diff(pred, n = 2, dim = 1) ** 2)\n",
    "    return base + beta * smoothness\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3, weight_decay = 1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 50)\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "patience = 30\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "best_model_path = 'best_model_20250801.pth'\n",
    "num_epoch = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for X_ts_batch, X_feat_batch, y_batch in train_loader:\n",
    "        X_ts_batch, X_feat_batch, y_batch = X_ts_batch.to(device), X_feat_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        preds = model(X_ts_batch, X_feat_batch)\n",
    "        loss = combined_loss(preds, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = max_grad_norm)\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_ts_batch, X_feat_batch, y_batch in val_loader:\n",
    "            X_ts_batch, X_feat_batch, y_batch = X_ts_batch.to(device), X_feat_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            preds = model(X_ts_batch, X_feat_batch)\n",
    "            val_loss = criterion(preds, y_batch)\n",
    "            total_val_loss += val_loss.item()\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epoch}], Train Loss: {avg_train_loss:.8f}, Val Loss: {avg_val_loss:.8f}\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print('Early Stopping Triggered')\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "start = 0\n",
    "target = len(val_dataset)\n",
    "n_cols = 3\n",
    "n_rows = (len(target) + n_cols - 1) // n_cols\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize = (7 * n_cols, 4 * n_rows), sharex = False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "model.eval()\n",
    "for i in tqdm(range(0, len(target)):\n",
    "    ax = axes[i]\n",
    "    index = target[i]\n",
    "    X_ts_sample, X_feat_sample, y_true_sample = val_dataset[index]\n",
    "    X_ts_sample = X_ts_sample.numpy().squeeze()\n",
    "    y_true_sample = y_true_sample.numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X_ts_tensor = torch.tensor(X_ts_sample, dtype = torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "        X_feat_tensor = X_feat_sample.unsqueeze(0).to(device)\n",
    "        y_pred_scaled = model(X_ts_tensor, X_feat_tensor).cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "    y_pred_log = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    y_true_log = scaler_y.inverse_transform(y_true_sample.reshape(-1, 1)).flatten()\n",
    "    y_true = np.expm1(y_true_log)\n",
    "\n",
    "    lookback_series = scaler_X.inverse_transform(X_ts_sample.reshape(-1, 1)).flatten()\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "    y_true_cumsum = np.cumsum(y_true)\n",
    "    y_pred_cumsum = np.cumsum(y_pred)\n",
    "\n",
    "    ax.plot(range(len(lookback_series)), lookback_series, label = 'History (Lookback)', color = 'blue')\n",
    "    ax.plot(range(len(lookback_series), len(lookback_series) + len(y_true)), y_true, label = 'Actual(Horizon)', color = 'green')\n",
    "    ax.plot(range(len(lookback_series), len(lookback_series) + len(y_pred)), y_pred, label = 'Predicted (Horizon)', color = 'red', linestyle = 'dashed')\n",
    "\n",
    "    ax.axvline(len(lookback_series) - 1, color = 'gray', linestyle = '--', alpha = 0.5)\n",
    "    ax.set_title(f\"sample {i} | True: {y_true.sum():.2f} PRED: {y_pred.sum():.2f}\\n {y_pred.sum() / y_true.sum() * 100}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    ax.yaxis.set_major_formatter(ScalarFormatter(useOffset = False))\n",
    "    ax.yaxis.get_major_formatter().set_scientific(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
