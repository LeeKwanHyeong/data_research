{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-02T02:43:05.812467Z",
     "start_time": "2025-08-02T02:43:05.636527Z"
    }
   },
   "source": [
    "import traceback\n",
    "\n",
    "import polars as pl\n",
    "import sys\n",
    "import torch\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from lifelines import WeibullFitter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils.lstf_feature_maker.piecewise_linear_regression import PiecewiseLinearRegression\n",
    "from utils.lstf_feature_maker.weibull import WeibullFeatureMaker\n",
    "\n",
    "'''\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "https://developer.nvidia.com/cuda-12-8-0-download-archive\n",
    "'''\n",
    "\n",
    "MAC_DIR = '../data/'\n",
    "WINDOW_DIR = 'C:/Users/USER/PycharmProjects/research/data/'\n",
    "\n",
    "if sys.platform == 'win32':\n",
    "    DIR = WINDOW_DIR\n",
    "    print(torch.cuda.is_available())\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.version.cuda)\n",
    "    print(torch.__version__)\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.__version__)\n",
    "else:\n",
    "    DIR = MAC_DIR\n",
    "\n",
    "tb_bas_oper_part_mst = (pl.read_parquet(DIR + 'tb_bas_oper_part_mst.parquet')\n",
    "                        .select(['OPER_PART_NO', 'OPER_PART_NM'])\n",
    "                        .rename({'OPER_PART_NO': 'oper_part_no', 'OPER_PART_NM': 'oper_part_nm'}))\n",
    "tb_dyn_fcst_demand = (pl.read_parquet(DIR + 'tb_dyn_fcst_dmnd.parquet')\n",
    "                      .select(['PART_NO', 'DMND_QTY', 'DMND_DT', 'OPER_PART_NO'])\n",
    "                      .rename({'PART_NO': 'part_no', 'OPER_PART_NO': 'oper_part_no', 'DMND_DT': 'demand_dt', 'DMND_QTY': 'demand_qty'})\n",
    "                      .select(['part_no', 'oper_part_no', 'demand_dt', 'demand_qty']))\n",
    "tb_dyn_fcst_demand_sellout = (pl.read_parquet(DIR + 'tb_dyn_fcst_dmnd_sellout.parquet')\n",
    "                              .select(['PART_NO', 'DMND_QTY', 'DMND_DT', 'OPER_PART_NO'])\n",
    "                              .rename({'PART_NO': 'part_no', 'OPER_PART_NO': 'oper_part_no', 'DMND_DT': 'demand_dt', 'DMND_QTY': 'demand_qty'})\n",
    "                              .select(['part_no', 'oper_part_no', 'demand_dt', 'demand_qty']))"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T02:43:24.322822Z",
     "start_time": "2025-08-02T02:43:12.812070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.date_util import DateUtil\n",
    "\n",
    "dyn_fcst_demand = tb_dyn_fcst_demand.with_columns([\n",
    "    pl.col('demand_dt').cast(pl.Int64).map_elements(DateUtil.yyyymmdd_to_date, return_dtype = pl.Date)\n",
    "])\n",
    "\n",
    "dyn_demand_sellout = tb_dyn_fcst_demand_sellout.with_columns([\n",
    "    pl.col('demand_dt').cast(pl.Int64).map_elements(DateUtil.yyyymmdd_to_date, return_dtype = pl.Date)\n",
    "])\n",
    "\n",
    "dyn_fcst_demand"
   ],
   "id": "99d9184d2ed28234",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (1_723_020, 4)\n",
       "┌───────────────┬───────────────┬────────────┬────────────┐\n",
       "│ part_no       ┆ oper_part_no  ┆ demand_dt  ┆ demand_qty │\n",
       "│ ---           ┆ ---           ┆ ---        ┆ ---        │\n",
       "│ str           ┆ str           ┆ date       ┆ f64        │\n",
       "╞═══════════════╪═══════════════╪════════════╪════════════╡\n",
       "│ T4240-71102BB ┆ T4240-71102BB ┆ 2018-01-01 ┆ 3.0        │\n",
       "│ T5210-34402   ┆ T5210-34402   ┆ 2018-01-01 ┆ 1.0        │\n",
       "│ T5210-30081   ┆ T5210-30081   ┆ 2018-01-01 ┆ 1.0        │\n",
       "│ T5210-65661   ┆ T5210-65661   ┆ 2018-01-01 ┆ 1.0        │\n",
       "│ T5210-66472   ┆ T5210-66472   ┆ 2018-01-01 ┆ 1.0        │\n",
       "│ …             ┆ …             ┆ …          ┆ …          │\n",
       "│ U3215-52203   ┆ U3215-52203   ┆ 2024-02-05 ┆ 30.0       │\n",
       "│ T5710-69252   ┆ T5710-69252   ┆ 2024-02-05 ┆ 2.0        │\n",
       "│ DYD1-O07      ┆ DYD1-O07      ┆ 2024-02-05 ┆ 4.0        │\n",
       "│ T2198-69775   ┆ T2198-69775   ┆ 2024-02-05 ┆ 6.0        │\n",
       "│ TC26-0416B    ┆ TC26-0416B    ┆ 2024-02-05 ┆ 2.0        │\n",
       "└───────────────┴───────────────┴────────────┴────────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_723_020, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>part_no</th><th>oper_part_no</th><th>demand_dt</th><th>demand_qty</th></tr><tr><td>str</td><td>str</td><td>date</td><td>f64</td></tr></thead><tbody><tr><td>&quot;T4240-71102BB&quot;</td><td>&quot;T4240-71102BB&quot;</td><td>2018-01-01</td><td>3.0</td></tr><tr><td>&quot;T5210-34402&quot;</td><td>&quot;T5210-34402&quot;</td><td>2018-01-01</td><td>1.0</td></tr><tr><td>&quot;T5210-30081&quot;</td><td>&quot;T5210-30081&quot;</td><td>2018-01-01</td><td>1.0</td></tr><tr><td>&quot;T5210-65661&quot;</td><td>&quot;T5210-65661&quot;</td><td>2018-01-01</td><td>1.0</td></tr><tr><td>&quot;T5210-66472&quot;</td><td>&quot;T5210-66472&quot;</td><td>2018-01-01</td><td>1.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;U3215-52203&quot;</td><td>&quot;U3215-52203&quot;</td><td>2024-02-05</td><td>30.0</td></tr><tr><td>&quot;T5710-69252&quot;</td><td>&quot;T5710-69252&quot;</td><td>2024-02-05</td><td>2.0</td></tr><tr><td>&quot;DYD1-O07&quot;</td><td>&quot;DYD1-O07&quot;</td><td>2024-02-05</td><td>4.0</td></tr><tr><td>&quot;T2198-69775&quot;</td><td>&quot;T2198-69775&quot;</td><td>2024-02-05</td><td>6.0</td></tr><tr><td>&quot;TC26-0416B&quot;</td><td>&quot;TC26-0416B&quot;</td><td>2024-02-05</td><td>2.0</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T02:43:26.659181Z",
     "start_time": "2025-08-02T02:43:26.015251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dyn_fcst = (dyn_fcst_demand\n",
    "                .join(tb_bas_oper_part_mst, on = 'oper_part_no', how = 'left')\n",
    "                .select(['oper_part_no', 'oper_part_nm', 'demand_dt','demand_qty'])\n",
    "                .sort(['oper_part_no', 'demand_dt'])\n",
    "                .with_columns([\n",
    "                    pl.col('demand_qty').cum_sum().over('oper_part_no').alias('cumsum_qty')\n",
    "                ])\n",
    "              )\n",
    "dyn_demand = (dyn_demand_sellout.join(tb_bas_oper_part_mst, on = 'oper_part_no', how = 'left')\n",
    "                    .select(['oper_part_no', 'oper_part_nm', 'demand_dt', 'demand_qty'])\n",
    "                    .sort(['oper_part_no', 'demand_dt'])\n",
    "                    .with_columns([\n",
    "                        pl.col('demand_qty').cum_sum().over('oper_part_no').alias('cumsum_qty')\n",
    "                 ])\n",
    "               )"
   ],
   "id": "d3d57d3b8aac857e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T02:43:31.148840Z",
     "start_time": "2025-08-02T02:43:31.142926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.lstf_feature_maker.exponential_smoothing import ExponentialSmoothing\n",
    "\n",
    "lookback_window = 40\n",
    "horizon = 20\n",
    "feature_schema = {\n",
    "    'oper_part_no': pl.Utf8,\n",
    "    'X_ts': pl.Array(pl.Float64, 40),\n",
    "    'y_ts': pl.List(pl.Float64),\n",
    "    'X_features': pl.List(pl.Float64)\n",
    "}\n",
    "\n",
    "def compute_feature_group(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    try:\n",
    "        part_no = df['oper_part_no'][0]\n",
    "        series = df['demand_qty'].to_numpy()\n",
    "\n",
    "        # lookback_window와 horizon 확인\n",
    "        if len(series) < lookback_window + horizon:\n",
    "            return pl.DataFrame(schema=feature_schema)\n",
    "\n",
    "        # Feature 계산\n",
    "        LTB_point = len(series) - horizon\n",
    "        slope_pre, slope_saddle, slope_post = PiecewiseLinearRegression(series = series, start_point = LTB_point).auto_piecewise_slopes()\n",
    "        k, lam = WeibullFeatureMaker().auto_weibull_params(series)\n",
    "\n",
    "        # Rolling Mean, SES\n",
    "        ma9 = df['demand_qty'].rolling_mean(9).to_list()\n",
    "        ses = ExponentialSmoothing().simple_exponential_smoothing(series = df['demand_qty'].to_list())\n",
    "\n",
    "        X_ts, y_ts, X_features = [], [], []\n",
    "        for i in range(len(series) - lookback_window - horizon - 1):\n",
    "            X_ts.append(series[i:i+lookback_window])\n",
    "            y_ts.append(series[i+lookback_window:i+lookback_window+horizon])\n",
    "            X_features.append([\n",
    "                slope_pre, slope_saddle, slope_post, k, lam,\n",
    "                ma9[i + lookback_window], ses[i + lookback_window]\n",
    "            ])\n",
    "\n",
    "        return pl.DataFrame({\n",
    "            'oper_part_no': [part_no] * len(X_ts),\n",
    "            'X_ts': X_ts,\n",
    "            'y_ts': y_ts,\n",
    "            'X_features': X_features\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_feature_group: {e}\")\n",
    "        print(traceback.print_exc())\n",
    "        # return pl.DataFrame(schema=feature_schema)  # 항상 동일 스키마 반환"
   ],
   "id": "4b97de7a56d5528f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T02:47:02.621025Z",
     "start_time": "2025-08-02T02:43:32.955494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "(dyn_demand\n",
    "    .select(['oper_part_no', 'demand_dt', 'demand_qty', 'cumsum_qty'])\n",
    "    .sort(['oper_part_no', 'demand_dt'])\n",
    "    .with_columns(pl.col('demand_dt').map_elements(DateUtil.date_to_yyyymm, return_dtype = pl.Int64).alias('demand_dt'))\n",
    "    .select('oper_part_no', 'demand_dt', 'demand_qty')\n",
    "    .group_by(['oper_part_no', 'demand_dt'])\n",
    "    .agg([\n",
    "        pl.col('demand_qty').sum().alias('demand_qty')\n",
    "    ])\n",
    "    .sort(['oper_part_no', 'demand_dt'])\n",
    "    .group_by('oper_part_no')\n",
    "    .map_groups(compute_feature_group)\n",
    " )\n"
   ],
   "id": "9fcaa88d6ea4e6a5",
   "outputs": [
    {
     "ename": "SchemaError",
     "evalue": "type Array(Float64, 40) is incompatible with expected type List(Float64)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mSchemaError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m      1\u001B[39m (\u001B[43mdyn_demand\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43moper_part_no\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdemand_dt\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdemand_qty\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mcumsum_qty\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43msort\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43moper_part_no\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdemand_dt\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mwith_columns\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpl\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdemand_dt\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_elements\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDateUtil\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdate_to_yyyymm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dtype\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mpl\u001B[49m\u001B[43m.\u001B[49m\u001B[43mInt64\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43malias\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdemand_dt\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43moper_part_no\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdemand_dt\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdemand_qty\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mgroup_by\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43moper_part_no\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdemand_dt\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43magg\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpl\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdemand_qty\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43malias\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdemand_qty\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43msort\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43moper_part_no\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdemand_dt\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mgroup_by\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43moper_part_no\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_groups\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcompute_feature_group\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m  )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/polars/dataframe/group_by.py:313\u001B[39m, in \u001B[36mGroupBy.map_groups\u001B[39m\u001B[34m(self, function)\u001B[39m\n\u001B[32m    309\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mcannot call `map_groups` when grouping by an expression\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    310\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[32m    312\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.df.\u001B[34m__class__\u001B[39m._from_pydf(\n\u001B[32m--> \u001B[39m\u001B[32m313\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_df\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgroup_by_map_groups\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    314\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mby\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmaintain_order\u001B[49m\n\u001B[32m    315\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    316\u001B[39m )\n",
      "\u001B[31mSchemaError\u001B[39m: type Array(Float64, 40) is incompatible with expected type List(Float64)"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9e9d82f46d01c4cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "lookback = 52\n",
    "horizon = 12\n",
    "\n",
    "X_ts, y_ts, X_features = [], [], []\n",
    "\n",
    "def compute_piecewise_slopes(values, LTB_point):\n",
    "    x1 = np.arange(LTB_point).reshape(-1, 1)\n",
    "    y1 = values[:LTB_point]\n",
    "    slope1 = LinearRegression().fit(x1, y1).coef_[0]\n",
    "\n",
    "    x2 = np.arange(LTB_point, len(values)).reshape(-1, 1)\n",
    "    y2 = values[LTB_point:]\n",
    "    slope2 = LinearRegression().fit(x2, y2).coef_[0]\n",
    "    return slope1, slope2\n",
    "\n",
    "def compute_weibull_params(values):\n",
    "    failure_times = np.repeat(np.arange(1, len(values)+1), values.astype(int))\n",
    "    if len(failure_times) < 5:\n",
    "        return 0.0, 0.0\n",
    "    wf = WeibullFitter()\n",
    "    wf.fit(failure_times, event_observed=[1]*len(failure_times))\n",
    "    return float(wf.rho_), float(wf.lambda_)\n",
    "\n",
    "for part_no in df_grouped['oper_part_no'].unique():\n",
    "    part_df = df_grouped.filter(pl.col(\"oper_part_no\") == part_no).sort(\"yyyyww\")\n",
    "\n",
    "    series = part_df['demand_qty'].to_numpy()\n",
    "    if len(series) < lookback + horizon:\n",
    "        continue  # 데이터 부족 시 스킵\n",
    "\n",
    "    # Feature 생성 (LTB 기준)\n",
    "    LTB_point = len(series) - horizon\n",
    "    slope_pre, slope_post = compute_piecewise_slopes(series, LTB_point)\n",
    "    k, lam = compute_weibull_params(series)\n",
    "\n",
    "    # 슬라이딩 윈도우 생성\n",
    "    for i in tqdm(range(len(series) - lookback - horizon + 1)):\n",
    "        X_ts.append(series[i:i+lookback])\n",
    "        y_ts.append(series[i+lookback:i+lookback+horizon])\n",
    "        X_features.append([slope_pre, slope_post, k, lam])\n",
    "\n",
    "X_ts = np.array(X_ts)\n",
    "y_ts = np.array(y_ts)\n",
    "X_features = np.array(X_features)\n",
    "\n",
    "print(f\"X_ts shape: {X_ts.shape}, y_ts shape: {y_ts.shape}, X_features shape: {X_features.shape}\")"
   ],
   "id": "76b3a48d66c3b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# scaler_ts = MinMaxScaler()\n",
    "# X_ts_scaled = scaler_ts.fit_transform(X_ts.reshape(-1, 1)).reshape(X_ts.shape)\n",
    "# y_ts_scaled = scaler_ts.transform(y_ts.reshape(-1, 1)).reshape(y_ts.shape)\n",
    "#\n",
    "# scaler_feat = StandardScaler()\n",
    "# X_features_scaled = scaler_feat.fit_transform(X_features)\n",
    "#\n",
    "# print(X_ts_scaled)\n",
    "# print(y_ts_scaled)\n",
    "# print(X_features_scaled)\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "X_ts_scaled = scaler_x.fit_transform(X_ts.reshape(-1, 1)).reshape(X_ts.shape)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_ts_scaled = scaler_y.fit_transform(y_ts.reshape(-1, 1)).reshape(y_ts.shape)\n",
    "\n",
    "scaler_feat = StandardScaler()\n",
    "X_features_scaled = scaler_feat.fit_transform(X_features)"
   ],
   "id": "77cf32221519dc0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class DemandDataset(Dataset):\n",
    "    def __init__(self, X_ts, X_feat, y_ts):\n",
    "        self.X_ts = torch.tensor(X_ts, dtype = torch.float32).unsqueeze(-1)\n",
    "        self.X_feat = torch.tensor(X_feat, dtype = torch.float32)\n",
    "        self.y_ts = torch.tensor(y_ts, dtype = torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_ts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_ts[idx], self.X_feat[idx], self.y_ts[idx]\n",
    "\n",
    "X_train, X_val, F_train, F_val, y_train, y_val = train_test_split(X_ts_scaled, X_features_scaled, y_ts_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_dataset = DemandDataset(X_train, F_train, y_train)\n",
    "val_dataset = DemandDataset(X_val, F_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 32, shuffle = False)"
   ],
   "id": "6cc18f92c8fa32b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from layers.RevIN import RevIN\n",
    "\n",
    "\n",
    "# 기존 PatchMixerLayer 그대로 사용\n",
    "class PatchMixerLayer(nn.Module):\n",
    "    def __init__(self, dim, a, kernel_size=8):\n",
    "        super().__init__()\n",
    "        self.Resnet = nn.Sequential(\n",
    "            nn.Conv1d(dim, dim, kernel_size=kernel_size, groups=dim, padding='same'),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(dim)\n",
    "        )\n",
    "        self.Conv_1x1 = nn.Sequential(\n",
    "            nn.Conv1d(dim, a, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(a)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x += self.Resnet(x)\n",
    "        x = self.Conv_1x1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# PatchMixer Backbone\n",
    "class PatchMixerBackbone(nn.Module):\n",
    "    def __init__(self, configs, revin=True, affine=True, subtract_last=False):\n",
    "        super().__init__()\n",
    "        self.n_vals = configs.enc_in\n",
    "        self.lookback = configs.seq_len\n",
    "        self.forecasting = configs.pred_len\n",
    "        self.patch_size = configs.patch_len\n",
    "        self.stride = configs.stride\n",
    "        self.kernel_size = configs.mixer_kernel_size\n",
    "\n",
    "        self.PatchMixer_blocks = nn.ModuleList([])\n",
    "        self.padding_patch_layer = nn.ReplicationPad1d((0, self.stride))\n",
    "        self.patch_num = int((self.lookback - self.patch_size) / self.stride + 1) + 1\n",
    "        self.a = self.patch_num\n",
    "        self.d_model = configs.d_model\n",
    "        self.dropout_rate = configs.head_dropout\n",
    "        self.depth = configs.e_layers\n",
    "\n",
    "        for _ in range(self.depth):\n",
    "            self.PatchMixer_blocks.append(\n",
    "                PatchMixerLayer(dim=self.patch_num, a=self.a, kernel_size=self.kernel_size)\n",
    "            )\n",
    "\n",
    "        self.W_P = nn.Linear(self.patch_size, self.d_model)\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        self.revin = revin\n",
    "        if self.revin:\n",
    "            self.revin_layer = RevIN(self.n_vals, affine=affine, subtract_last=subtract_last)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        n_vars = x.shape[-1]\n",
    "\n",
    "        if self.revin:\n",
    "            x = self.revin_layer(x, 'norm')\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x_lookback = self.padding_patch_layer(x)\n",
    "        x = x_lookback.unfold(dimension=-1, size=self.patch_size, step=self.stride)\n",
    "\n",
    "        x = self.W_P(x)\n",
    "        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for PatchMixer_block in self.PatchMixer_blocks:\n",
    "            x = PatchMixer_block(x)\n",
    "\n",
    "        # Global representation (flatten for merge)\n",
    "        x = self.flatten(x)  # shape: (batch*n_vars, a*d_model)\n",
    "        x = x.view(bs, n_vars, -1)\n",
    "        x = x.mean(dim=1)  # aggregate across variables\n",
    "        return x  # shape: (batch, patch_num*d_model)\n",
    "\n",
    "\n",
    "# Full Model: PatchMixer + Feature Branch\n",
    "class PatchMixerFeatureModel(nn.Module):\n",
    "    def __init__(self, configs, feature_dim=4):\n",
    "        super().__init__()\n",
    "        self.backbone = PatchMixerBackbone(configs)\n",
    "\n",
    "        # Feature Branch\n",
    "        self.feature_mlp = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Combined Head\n",
    "        patch_repr_dim = self.backbone.a * self.backbone.d_model\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(patch_repr_dim + 8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, configs.pred_len)  # output = horizon length\n",
    "        )\n",
    "\n",
    "    def forward(self, ts_input, feature_input):\n",
    "        patch_repr = self.backbone(ts_input)\n",
    "        feature_repr = self.feature_mlp(feature_input)\n",
    "        combined = torch.cat([patch_repr, feature_repr], dim=1)\n",
    "        out = self.fc(combined)\n",
    "        return out"
   ],
   "id": "36777eb6fb19df01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.version.cuda)\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.__version__)"
   ],
   "id": "578b129d42a7c924",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "class Configs:\n",
    "    enc_in = 1\n",
    "    seq_len = lookback\n",
    "    pred_len = horizon\n",
    "    patch_len = 16\n",
    "    stride = 8\n",
    "    mixer_kernel_size = 8\n",
    "    d_model = 32\n",
    "    head_dropout = 0.1\n",
    "    e_layers = 3\n",
    "\n",
    "configs = Configs()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = PatchMixerFeatureModel(configs).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "for epoch in tqdm(range(5)):\n",
    "    print('epoch', epoch)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_ts_batch, X_feat_batch, y_batch in train_loader:\n",
    "        X_ts_batch, X_feat_batch, y_batch = X_ts_batch.to(device), X_feat_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_ts_batch, X_feat_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {total_loss / len(train_loader): .4f}\")\n"
   ],
   "id": "3818bf52774b3db6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for X_ts_batch, X_feat_batch, _ in val_loader:\n",
    "        X_ts_batch = X_ts_batch.to(device)\n",
    "        X_feat_batch = X_feat_batch.to(device)\n",
    "        preds = model(X_ts_batch, X_feat_batch)\n",
    "        predictions.append(preds.cpu().numpy())\n",
    "\n",
    "predictions = np.concatenate(predictions, axis = 0)\n",
    "predictions_inverse = scaler_y.inverse_transform(\n",
    "    predictions.reshape(-1, 1)\n",
    ").reshape(predictions.shape)\n",
    "\n",
    "weeks = [f'week_{i+1}' for i in range(horizon)]\n",
    "result_df = pl.DataFrame({weeks[i]: predictions_inverse[:, i] for i in range(horizon)})\n",
    "print('예측 결과 샘플')\n",
    "print(result_df)"
   ],
   "id": "dcf13a4dc1a3cefc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "result_df[1]",
   "id": "f82b6645b91fd3fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "result_df[2]",
   "id": "418fd95ea4d883b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "sample_idx = random.randint(0, len(val_dataset) - 1)\n",
    "\n",
    "X_ts_sample, X_feat_sample, y_true_sample = val_dataset[sample_idx]\n",
    "X_ts_sample = X_ts_sample.numpy().squeeze() # (Lookback, )\n",
    "y_true_sample = y_true_sample.numpy() # (horizon, )\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_ts_tensor = torch.tensor(X_ts_sample, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "    X_feat_tensor = torch.tensor(X_feat_sample, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    y_pred_scaled = model(X_ts_tensor, X_feat_tensor).cpu().numpy().flatten()\n",
    "\n",
    "y_true = scaler_y.inverse_transform(y_true_sample.reshape(-1, 1)).flatten()\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "lookback_series = scaler_x.inverse_transform(X_ts_sample.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "plt.figure(figsize = (12, 5))\n",
    "plt.plot(range(len(lookback_series)), lookback_series, label = 'History (Lookback)', color = 'blue')\n",
    "plt.plot(range(len(lookback_series), len(lookback_series) + len(y_true)), y_true, label = 'Actual (Horizon)', color = 'green')\n",
    "plt.plot(range(len(lookback_series), len(lookback_series) + len(y_pred)), y_pred, label = 'Predicted (Horizon)', color = 'red', linestyle = 'dashed')\n",
    "\n",
    "plt.axvline(len(lookback_series)-1, color = 'gray', linestyle = '--', alpha = 0.5)\n",
    "plt.title(f\"Sample #{sample_idx} - Actual vs Predicted (Horizon = {horizon})\")\n",
    "plt.xlabel('Time Steps (Weeks)')\n",
    "plt.ylabel('Demand Quantity')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "fd83824580c09ed6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "eb2ab5ebd4d6f163",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "eb71029c35a261f6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
