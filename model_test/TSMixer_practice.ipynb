{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-03T23:32:53.489396Z",
     "start_time": "2025-08-03T23:32:46.109485Z"
    }
   },
   "source": [
    "import traceback\n",
    "\n",
    "import polars as pl\n",
    "import sys\n",
    "import torch\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from lifelines import WeibullFitter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils.custom_losses import CustomLoss\n",
    "from utils.lstf_feature_maker.piecewise_linear_regression import PiecewiseLinearRegression\n",
    "from utils.lstf_feature_maker.weibull import WeibullFeatureMaker\n",
    "\n",
    "'''\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "https://developer.nvidia.com/cuda-12-8-0-download-archive\n",
    "'''\n",
    "\n",
    "MAC_DIR = '../data/'\n",
    "WINDOW_DIR = 'C:/Users/USER/PycharmProjects/research/data/'\n",
    "\n",
    "if sys.platform == 'win32':\n",
    "    DIR = WINDOW_DIR\n",
    "    print(torch.cuda.is_available())\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.version.cuda)\n",
    "    print(torch.__version__)\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.__version__)\n",
    "else:\n",
    "    DIR = MAC_DIR\n",
    "\n",
    "tb_bas_oper_part_mst = (pl.read_parquet(DIR + 'tb_bas_oper_part_mst.parquet')\n",
    "                        .select(['OPER_PART_NO', 'OPER_PART_NM'])\n",
    "                        .rename({'OPER_PART_NO': 'oper_part_no', 'OPER_PART_NM': 'oper_part_nm'}))\n",
    "tb_dyn_fcst_demand = (pl.read_parquet(DIR + 'tb_dyn_fcst_dmnd.parquet')\n",
    "                      .select(['PART_NO', 'DMND_QTY', 'DMND_DT', 'OPER_PART_NO'])\n",
    "                      .rename({'PART_NO': 'part_no', 'OPER_PART_NO': 'oper_part_no', 'DMND_DT': 'demand_dt', 'DMND_QTY': 'demand_qty'})\n",
    "                      .select(['part_no', 'oper_part_no', 'demand_dt', 'demand_qty']))\n",
    "tb_dyn_fcst_demand_sellout = (pl.read_parquet(DIR + 'tb_dyn_fcst_dmnd_sellout.parquet')\n",
    "                              .select(['PART_NO', 'DMND_QTY', 'DMND_DT', 'OPER_PART_NO'])\n",
    "                              .rename({'PART_NO': 'part_no', 'OPER_PART_NO': 'oper_part_no', 'DMND_DT': 'demand_dt', 'DMND_QTY': 'demand_qty'})\n",
    "                              .select(['part_no', 'oper_part_no', 'demand_dt', 'demand_qty']))"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T23:33:17.462578Z",
     "start_time": "2025-08-03T23:33:08.739500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.date_util import DateUtil\n",
    "\n",
    "dyn_fcst_demand = tb_dyn_fcst_demand.with_columns([\n",
    "    pl.col('demand_dt').cast(pl.Int64).map_elements(DateUtil.yyyymmdd_to_date, return_dtype = pl.Date)\n",
    "])\n",
    "\n",
    "dyn_demand_sellout = tb_dyn_fcst_demand_sellout.with_columns([\n",
    "    pl.col('demand_dt').cast(pl.Int64).map_elements(DateUtil.yyyymmdd_to_date, return_dtype = pl.Date)\n",
    "])\n",
    "\n",
    "dyn_fcst_demand"
   ],
   "id": "99d9184d2ed28234",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (1_723_020, 4)\n",
       "┌───────────────┬───────────────┬────────────┬────────────┐\n",
       "│ part_no       ┆ oper_part_no  ┆ demand_dt  ┆ demand_qty │\n",
       "│ ---           ┆ ---           ┆ ---        ┆ ---        │\n",
       "│ str           ┆ str           ┆ date       ┆ f64        │\n",
       "╞═══════════════╪═══════════════╪════════════╪════════════╡\n",
       "│ T4240-71102BB ┆ T4240-71102BB ┆ 2018-01-01 ┆ 3.0        │\n",
       "│ T5210-34402   ┆ T5210-34402   ┆ 2018-01-01 ┆ 1.0        │\n",
       "│ T5210-30081   ┆ T5210-30081   ┆ 2018-01-01 ┆ 1.0        │\n",
       "│ T5210-65661   ┆ T5210-65661   ┆ 2018-01-01 ┆ 1.0        │\n",
       "│ T5210-66472   ┆ T5210-66472   ┆ 2018-01-01 ┆ 1.0        │\n",
       "│ …             ┆ …             ┆ …          ┆ …          │\n",
       "│ U3215-52203   ┆ U3215-52203   ┆ 2024-02-05 ┆ 30.0       │\n",
       "│ T5710-69252   ┆ T5710-69252   ┆ 2024-02-05 ┆ 2.0        │\n",
       "│ DYD1-O07      ┆ DYD1-O07      ┆ 2024-02-05 ┆ 4.0        │\n",
       "│ T2198-69775   ┆ T2198-69775   ┆ 2024-02-05 ┆ 6.0        │\n",
       "│ TC26-0416B    ┆ TC26-0416B    ┆ 2024-02-05 ┆ 2.0        │\n",
       "└───────────────┴───────────────┴────────────┴────────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_723_020, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>part_no</th><th>oper_part_no</th><th>demand_dt</th><th>demand_qty</th></tr><tr><td>str</td><td>str</td><td>date</td><td>f64</td></tr></thead><tbody><tr><td>&quot;T4240-71102BB&quot;</td><td>&quot;T4240-71102BB&quot;</td><td>2018-01-01</td><td>3.0</td></tr><tr><td>&quot;T5210-34402&quot;</td><td>&quot;T5210-34402&quot;</td><td>2018-01-01</td><td>1.0</td></tr><tr><td>&quot;T5210-30081&quot;</td><td>&quot;T5210-30081&quot;</td><td>2018-01-01</td><td>1.0</td></tr><tr><td>&quot;T5210-65661&quot;</td><td>&quot;T5210-65661&quot;</td><td>2018-01-01</td><td>1.0</td></tr><tr><td>&quot;T5210-66472&quot;</td><td>&quot;T5210-66472&quot;</td><td>2018-01-01</td><td>1.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;U3215-52203&quot;</td><td>&quot;U3215-52203&quot;</td><td>2024-02-05</td><td>30.0</td></tr><tr><td>&quot;T5710-69252&quot;</td><td>&quot;T5710-69252&quot;</td><td>2024-02-05</td><td>2.0</td></tr><tr><td>&quot;DYD1-O07&quot;</td><td>&quot;DYD1-O07&quot;</td><td>2024-02-05</td><td>4.0</td></tr><tr><td>&quot;T2198-69775&quot;</td><td>&quot;T2198-69775&quot;</td><td>2024-02-05</td><td>6.0</td></tr><tr><td>&quot;TC26-0416B&quot;</td><td>&quot;TC26-0416B&quot;</td><td>2024-02-05</td><td>2.0</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T23:33:34.913088Z",
     "start_time": "2025-08-03T23:33:34.426177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dyn_fcst = (dyn_fcst_demand\n",
    "                .join(tb_bas_oper_part_mst, on = 'oper_part_no', how = 'left')\n",
    "                .select(['oper_part_no', 'oper_part_nm', 'demand_dt','demand_qty'])\n",
    "                .sort(['oper_part_no', 'demand_dt'])\n",
    "                .with_columns([\n",
    "                    pl.col('demand_qty').cum_sum().over('oper_part_no').alias('cumsum_qty')\n",
    "                ])\n",
    "              )\n",
    "dyn_demand = (dyn_demand_sellout.join(tb_bas_oper_part_mst, on = 'oper_part_no', how = 'left')\n",
    "                    .select(['oper_part_no', 'oper_part_nm', 'demand_dt', 'demand_qty'])\n",
    "                    .sort(['oper_part_no', 'demand_dt'])\n",
    "                    .with_columns([\n",
    "                        pl.col('demand_qty').cum_sum().over('oper_part_no').alias('cumsum_qty')\n",
    "                 ])\n",
    "               )"
   ],
   "id": "d3d57d3b8aac857e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T23:36:34.462552Z",
     "start_time": "2025-08-03T23:36:34.458129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.lstf_feature_maker.exponential_smoothing import ExponentialSmoothing\n",
    "\n",
    "lookback_window = 40\n",
    "horizon = 20\n",
    "feature_schema = {\n",
    "    'oper_part_no': pl.Utf8,\n",
    "    'X_ts': pl.Array(pl.Float64, 40),\n",
    "    'y_ts': pl.Array(pl.Float64, 20),\n",
    "    'X_features': pl.List(pl.Float64)\n",
    "}\n",
    "\n",
    "def compute_feature_group(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    try:\n",
    "        part_no = df['oper_part_no'][0]\n",
    "        series = df['demand_qty'].to_numpy()\n",
    "\n",
    "        # lookback_window와 horizon 확인\n",
    "        if len(series) < lookback_window + horizon:\n",
    "            return pl.DataFrame(schema=feature_schema)\n",
    "\n",
    "        # Feature 계산\n",
    "        LTB_point = len(series) - horizon\n",
    "        slope_pre, slope_saddle, slope_post = PiecewiseLinearRegression(series = series, start_point = LTB_point).auto_piecewise_slopes()\n",
    "        k, lam = WeibullFeatureMaker().auto_weibull_params(series)\n",
    "\n",
    "        # Rolling Mean, SES\n",
    "        ma9 = df['demand_qty'].rolling_mean(9).to_list()\n",
    "        ses = ExponentialSmoothing().simple_exponential_smoothing(series = df['demand_qty'].to_list())\n",
    "\n",
    "        X_ts, y_ts, X_features = [], [], []\n",
    "        for i in range(len(series) - lookback_window - horizon - 1):\n",
    "            X_ts.append(series[i:i+lookback_window])\n",
    "            y_ts.append(series[i+lookback_window:i+lookback_window+horizon])\n",
    "            X_features.append([\n",
    "                slope_pre, slope_saddle, slope_post, k, lam,\n",
    "                ma9[i + lookback_window], ses[i + lookback_window]\n",
    "            ])\n",
    "\n",
    "        return pl.DataFrame({\n",
    "            'oper_part_no': [part_no] * len(X_ts),\n",
    "            'X_ts': X_ts,\n",
    "            'y_ts': y_ts,\n",
    "            'X_features': X_features\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_feature_group: {e}\")\n",
    "        print(traceback.print_exc())\n",
    "        # return pl.DataFrame(schema=feature_schema)  # 항상 동일 스키마 반환"
   ],
   "id": "4b97de7a56d5528f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T23:42:29.689560Z",
     "start_time": "2025-08-03T23:39:52.541205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_computed_grouped = (dyn_demand\n",
    "    .select(['oper_part_no', 'demand_dt', 'demand_qty', 'cumsum_qty'])\n",
    "    .sort(['oper_part_no', 'demand_dt'])\n",
    "    .with_columns(pl.col('demand_dt').map_elements(DateUtil.date_to_yyyymm, return_dtype = pl.Int64).alias('demand_dt'))\n",
    "    .select('oper_part_no', 'demand_dt', 'demand_qty')\n",
    "    .group_by(['oper_part_no', 'demand_dt'])\n",
    "    .agg([\n",
    "        pl.col('demand_qty').sum().alias('demand_qty')\n",
    "    ])\n",
    "    .sort(['oper_part_no', 'demand_dt'])\n",
    "    .group_by('oper_part_no')\n",
    "    .map_groups(compute_feature_group)\n",
    " )\n"
   ],
   "id": "9fcaa88d6ea4e6a5",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T23:58:56.769240Z",
     "start_time": "2025-08-03T23:58:56.763442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# df_computed_grouped.write_parquet(DIR + '\\\\patch_mixer_parquets\\\\L_40_H_20_master.parquet')\n",
    "df_computed_grouped['X_features']"
   ],
   "id": "9e9d82f46d01c4cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (112_760,)\n",
       "Series: 'X_features' [list[f64]]\n",
       "[\n",
       "\t[-13.424869, 4.5, … 17.433615]\n",
       "\t[-13.424869, 4.5, … 15.20353]\n",
       "\t[-13.424869, 4.5, … 10.942471]\n",
       "\t[-13.424869, 4.5, … 10.65973]\n",
       "\t[-0.03005, 6.1, … 4.471545]\n",
       "\t…\n",
       "\t[0.0, 0.0, … 2687.841903]\n",
       "\t[0.0, 0.0, … 2067.489332]\n",
       "\t[0.0, 0.0, … 2950.542532]\n",
       "\t[0.0, 0.0, … 2509.379773]\n",
       "\t[0.0, 0.0, … 1858.565841]\n",
       "]"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (112_760,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>X_features</th></tr><tr><td>list[f64]</td></tr></thead><tbody><tr><td>[-13.424869, 4.5, … 17.433615]</td></tr><tr><td>[-13.424869, 4.5, … 15.20353]</td></tr><tr><td>[-13.424869, 4.5, … 10.942471]</td></tr><tr><td>[-13.424869, 4.5, … 10.65973]</td></tr><tr><td>[-0.03005, 6.1, … 4.471545]</td></tr><tr><td>&hellip;</td></tr><tr><td>[0.0, 0.0, … 2687.841903]</td></tr><tr><td>[0.0, 0.0, … 2067.489332]</td></tr><tr><td>[0.0, 0.0, … 2950.542532]</td></tr><tr><td>[0.0, 0.0, … 2509.379773]</td></tr><tr><td>[0.0, 0.0, … 1858.565841]</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T23:56:05.548237Z",
     "start_time": "2025-08-03T23:56:04.886531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_ts = np.vstack(df_computed_grouped['X_ts'].to_list())\n",
    "y_ts = np.vstack(df_computed_grouped['y_ts'].to_list())\n",
    "X_features = np.vstack(df_computed_grouped['X_features'].to_list())"
   ],
   "id": "86e4b2785aefdb4",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T23:59:30.849425Z",
     "start_time": "2025-08-03T23:59:30.771809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "X_ts_scaled = scaler_X.fit_transform(X_ts.reshape(-1, 1)).reshape(X_ts.shape)\n",
    "\n",
    "y_ts_log = np.log1p(y_ts)\n",
    "scaler_y = MinMaxScaler()\n",
    "y_ts_scaled = scaler_y.fit_transform(y_ts_log.reshape(-1, 1)).reshape(y_ts_log.shape)\n",
    "\n",
    "scaler_feat = StandardScaler()\n",
    "X_features_scaled = scaler_feat.fit_transform(X_features)"
   ],
   "id": "77cf32221519dc0e",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T00:00:08.656976Z",
     "start_time": "2025-08-04T00:00:08.580553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DemandDataset(Dataset):\n",
    "    def __init__(self, X_ts, X_feat, y_ts):\n",
    "        self.X_ts = torch.tensor(X_ts, dtype = torch.float32).unsqueeze(-1)\n",
    "        self.X_feat = torch.tensor(X_feat, dtype = torch.float32)\n",
    "        self.y_ts = torch.tensor(y_ts, dtype = torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_ts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_ts[idx], self.X_feat[idx], self.y_ts[idx]\n",
    "\n",
    "X_train, X_val, F_train, F_val, y_train, y_val = train_test_split(X_ts_scaled, X_features_scaled, y_ts_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_dataset = DemandDataset(X_train, F_train, y_train)\n",
    "val_dataset = DemandDataset(X_val, F_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 32, shuffle = False)"
   ],
   "id": "6cc18f92c8fa32b8",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T00:11:52.821256Z",
     "start_time": "2025-08-04T00:11:52.002142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.PatchMixer import PatchMixerFeatureModel\n",
    "class Config:\n",
    "    enc_in = 1\n",
    "    seq_len = lookback_window\n",
    "    pred_len = horizon\n",
    "    patch_len = 16\n",
    "    stride = 8\n",
    "    mixer_kernel_size = 8\n",
    "    d_model = 16\n",
    "    head_dropout = 0.1\n",
    "    e_layers = 2\n",
    "\n",
    "config = Config()\n",
    "\n",
    "if sys.platform == 'win32':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    device = torch.device('mps' if torch.mps.is_available() else 'cpu')\n",
    "\n",
    "model = PatchMixerFeatureModel(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3, weight_decay = 1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 50)\n",
    "max_grad_norm = 1.0\n",
    "patience = 30\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "best_model_path = 'best_model_20250804.pth'\n",
    "num_epoch = 5000"
   ],
   "id": "7cfa77fff266b31d",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for X_ts_batch, X_feat_batch, y_batch in train_loader:\n",
    "        X_ts_batch, X_feat_batch, y_batch = X_ts_batch.to(device), X_feat_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        pred = model(X_ts_batch, X_feat_batch)\n",
    "        loss = CustomLoss(pred, y_batch).combined_loss()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = max_grad_norm)\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_ts_batch, X_feat_batch, y_batch in val_loader:\n",
    "            X_ts_batch, X_feat_batch, y_batch = X_ts_batch.to(device), X_feat_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            pred = model(X_ts_batch, X_feat_batch)\n",
    "            val_loss = CustomLoss(pred, y_batch).combined_loss()\n",
    "            total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epoch}], Train Loss: {avg_train_loss:.8f}, Val Loss: {avg_val_loss:.8f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print('Early Stopping Triggered')\n",
    "                break\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path))"
   ],
   "id": "eb2ab5ebd4d6f163",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "eb71029c35a261f6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
